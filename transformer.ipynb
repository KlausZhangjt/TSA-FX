{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFIkuZWkBfjT",
        "outputId": "dbed4232-9460-440d-9817-fd30173351f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "dirpath = \"/content/drive/MyDrive/5053/\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import time\n",
        "import math\n",
        "from matplotlib import pyplot\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "\n",
        "calculate_loss_over_all_values = False\n",
        "input_window = 45\n",
        "output_window = 5\n",
        "batch_size = 20 # batch size\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "Xwg4zd5MBu_2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af7aa843-9f67-4ef0-ec3b-e14f8c632cb5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(d,i,num):\n",
        "    \"\"\"用于获得每个批次合理大小的源数据和目标数据.\n",
        "       参数source是通过batchify得到的train_data/val_data/test_data.\n",
        "       i是具体的批次次数.\n",
        "    \"\"\"\n",
        "\n",
        "    # 首先我们确定句子长度, 它将是在bptt和len(source) - 1 - i中最小值\n",
        "    # 实质上, 前面的批次中都会是bptt的值, 只不过最后一个批次中, 句子长度\n",
        "    # 可能不够bptt的35个, 因此会变为len(source) - 1 - i的值.\n",
        "    seq_len = num\n",
        "\n",
        "    # 语言模型训练的源数据的第i批数据将是batchify的结果的切片[i:i+seq_len]\n",
        "\n",
        "    train0 = d[i:i+seq_len]\n",
        "\n",
        "\n",
        "    # 根据语言模型训练的语料规定, 它的目标数据是源数据向后移动一位\n",
        "    # 因为最后目标数据的切片会越界, 因此使用view(-1)来保证形状正常.\n",
        "\n",
        "    test0 = d[i+1:i+1+seq_len]\n",
        "    return train0,test0\n",
        "class PositionalEncoding(nn.Module):\n",
        " \n",
        "    def __init__(self, d_model, dropout=0, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        " \n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        " \n",
        "    def forward(self, x):\n",
        "#         self.pe = self.pe.expand(-1, batch_size, -1)\n",
        "        x = x.expand(-1, -1, 512) + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)\n",
        "class TransAm(nn.Module):\n",
        "    def __init__(self, feature_size=512, num_layers=1, dropout=0):  # feature_size 表示特征维度（必须是head的整数倍）, num_layers 表示 Encoder_layer 的层数， dropout 用于防止过你和\n",
        "        super(TransAm, self).__init__()\n",
        "        self.model_type = 'Transformer'\n",
        "        self.src_mask = None\n",
        "        self.pos_encoder = PositionalEncoding(feature_size,0)  #位置编码前要做归一化，否则捕获不到位置信息\n",
        "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=feature_size, nhead=8,dim_feedforward = 64, dropout=dropout)  # 这里用了八个头\n",
        "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
        "        self.decoder = nn.Linear(feature_size, 1)  # 这里用全连接层代替了decoder， 其实也可以加一下Transformer的decoder试一下效果\n",
        "        self.init_weights()\n",
        "    def init_weights(self):  \n",
        "        initrange = 0.1\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, src):\n",
        "#         if self.src_key_padding_mask is None:\n",
        "#             mask_key = src_padding.bool()\n",
        "#             self.src_key_padding_mask = mask_key\n",
        "        src = self.pos_encoder(src)\n",
        "        src = src.transpose(0,1)\n",
        "        output = self.transformer_encoder(src, src_key_padding_mask=self.src_mask)  \n",
        "        output = self.decoder(output).view(10,-1)\n",
        "        return output\n",
        "\n",
        "class TransformerForAUD:\n",
        "  def read_data(self, path):\n",
        "    data = pd.read_csv(dirpath + 'AUD_v2.csv')\n",
        "    self.raw_value = data['tri'].to_numpy()\n",
        "\n",
        "  def preprocess(self):\n",
        "    tri = self.raw_value\n",
        "    # diff\n",
        "    tri = tri[1:]\n",
        "    tri = np.diff(tri)\n",
        "    self.tri = tri\n",
        "\n",
        "    # \n",
        "    self.device = device\n",
        "    self.model = TransAm()\n",
        "    self.model = self.model.to(device)\n",
        "    self.criterion = nn.MSELoss()\n",
        "    self.lr = 0.00000001  # 学习率最好设置的小一些，太大的话loss会出现nan的情况\n",
        "    # optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "    self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.lr)\n",
        "    self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, 3, gamma=0.96)\n",
        "\n",
        "  \n",
        "  def train(self):\n",
        "    self.model.train()  # Turn on the train mode\n",
        "\n",
        "    for epoch in range(10):\n",
        "      total_loss = 0.\n",
        "      start_time = time.time()\n",
        "\n",
        "      for batch, i in enumerate(range(0, 3000, 1)):\n",
        "          train_0,test_0 = get_batch(self.tri,i,10)\n",
        "          train_0 = torch.from_numpy(train_0)\n",
        "          train_0 = train_0.float()\n",
        "          train_0 = torch.unsqueeze(train_0,1)\n",
        "          train_0 = train_0.unsqueeze(2)\n",
        "          test_0 = torch.from_numpy(test_0)\n",
        "          test_0 = test_0.float()\n",
        "          self.optimizer.zero_grad()\n",
        "\n",
        "          train_0 = train_0.to(device)\n",
        "          test_0 = test_0.to(device=device)\n",
        "\n",
        "          output = self.model(train_0)\n",
        "          loss = self.criterion(output.view(-1), test_0)\n",
        "          loss.backward()\n",
        "          torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.5)\n",
        "          self.optimizer.step()\n",
        "\n",
        "          total_loss += loss.item()\n",
        "      print(f\"epoch {epoch}, total_loss = {total_loss}\")\n",
        "\n",
        "  # def test(self):"
      ],
      "metadata": {
        "id": "TvtAx7YFvOui"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer = TransformerForAUD()"
      ],
      "metadata": {
        "id": "45CrIejpxw3H"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.read_data(dirpath + 'AUD_v2.csv')\n",
        "transformer.preprocess()\n",
        "print(transformer.device)\n",
        "print(next(transformer.model.parameters()).device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VUYP_eWx9qf",
        "outputId": "320b224d-4b3a-41f8-9ae6-9903160d67c2"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvhYT4MnyLRa",
        "outputId": "32cb7a74-8dad-4098-df02-eaa982f8f95c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0, total_loss = 1858.6368730720133\n",
            "epoch 1, total_loss = 1694.780916839838\n",
            "epoch 2, total_loss = 1627.7173247411847\n",
            "epoch 3, total_loss = 1584.2653926350176\n",
            "epoch 4, total_loss = 1553.7855370752513\n",
            "epoch 5, total_loss = 1531.9631012063473\n",
            "epoch 6, total_loss = 1516.0573090314865\n",
            "epoch 7, total_loss = 1504.2159503456205\n",
            "epoch 8, total_loss = 1495.223971599713\n",
            "epoch 9, total_loss = 1488.2567310575396\n"
          ]
        }
      ]
    }
  ]
}